{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36cee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "class ABODatasetSimplified(Dataset):\n",
    "    \"\"\"\n",
    "    Further simplified PyTorch Dataset for loading ABO data.\n",
    "    Focuses on core loading steps, assuming data is mostly well-formed.\n",
    "\n",
    "    Args:\n",
    "        data_root (str): Path to the root directory.\n",
    "        n_ctx (int): Target number of points for the point cloud.\n",
    "        clip_preprocessor (callable): Preprocessing function for CLIP images.\n",
    "        image_subdir (str): Optional subdirectory for images.\n",
    "        pc_keys (tuple): Keys for coordinates and colors in NPZ files.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_root, n_ctx, clip_preprocessor, image_subdir='', pc_keys=('coords', 'colors')):\n",
    "        super().__init__()\n",
    "        self.data_root = data_root\n",
    "        self.n_ctx = n_ctx\n",
    "        self.image_subdir = image_subdir\n",
    "        self.pc_keys = pc_keys\n",
    "        self.clip_preprocess = clip_preprocessor\n",
    "\n",
    "        self.captions_dir = os.path.join(data_root, 'captions')\n",
    "        self.images_dir = os.path.join(data_root, 'images')\n",
    "        self.pointclouds_dir = os.path.join(data_root, 'pointclouds')\n",
    "        self.jsonl_path = os.path.join(data_root, 'final_dataset.jsonl')\n",
    "\n",
    "        if not os.path.exists(self.jsonl_path):\n",
    "            raise FileNotFoundError(f\"Error: JSONL file not found at {self.jsonl_path}\")\n",
    "\n",
    "        self.uids = self._load_uids()\n",
    "        if not self.uids:\n",
    "            raise ValueError(f\"No UIDs loaded from {self.jsonl_path}.\")\n",
    "        print(f\"Found {len(self.uids)} UIDs in jsonl file.\")\n",
    "\n",
    "    def _load_uids(self):\n",
    "        \"\"\" Loads UIDs from the JSON Lines file. \"\"\"\n",
    "        uids = []\n",
    "        with open(self.jsonl_path, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line.strip())\n",
    "                    uid = None\n",
    "                    if isinstance(data, dict) and 'uid' in data:\n",
    "                        uid = data['uid']\n",
    "                    elif isinstance(data, str):\n",
    "                         uid = data\n",
    "                    if uid:\n",
    "                        uids.append(uid)\n",
    "                except json.JSONDecodeError:\n",
    "                     pass # Silently ignore invalid JSON lines\n",
    "        return uids\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the total number of samples. \"\"\"\n",
    "        return len(self.uids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Loads and returns a single data sample. Minimal error checking. \"\"\"\n",
    "        uid = self.uids[idx]\n",
    "        caption = \"\" # Default empty caption\n",
    "        image_tensor = torch.zeros((3, 224, 224)) # Default blank image tensor\n",
    "        pointcloud_tensor = torch.zeros((6, self.n_ctx)) # Default blank point cloud tensor\n",
    "\n",
    "        try:\n",
    "            # --- Load Caption (optional) ---\n",
    "            caption_path = os.path.join(self.captions_dir, f\"{uid}.txt\")\n",
    "            if os.path.exists(caption_path):\n",
    "                 with open(caption_path, 'r', encoding='utf-8') as f:\n",
    "                    caption = f.read().strip()\n",
    "\n",
    "            # --- Load Image ---\n",
    "            image_base_path = os.path.join(self.images_dir, uid)\n",
    "            if self.image_subdir:\n",
    "                 image_base_path = os.path.join(image_base_path, self.image_subdir)\n",
    "\n",
    "            image_path = None\n",
    "            possible_image_paths = glob.glob(f\"{image_base_path}.*\")\n",
    "            if not possible_image_paths:\n",
    "                 possible_image_paths = glob.glob(os.path.join(image_base_path, '*.*'))\n",
    "\n",
    "            img_extensions = ['.png', '.jpg', '.jpeg', '.webp']\n",
    "            for path in possible_image_paths:\n",
    "                if any(path.lower().endswith(ext) for ext in img_extensions):\n",
    "                    image_path = path\n",
    "                    break\n",
    "\n",
    "            if image_path: # Only process if an image path was found\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image_tensor = self.clip_preprocess(image)\n",
    "            # else: image_tensor remains zeros if no path found\n",
    "\n",
    "            # --- Load Point Cloud ---\n",
    "            pc_path = os.path.join(self.pointclouds_dir, f\"{uid}.npz\")\n",
    "            if os.path.exists(pc_path): # Only process if npz file exists\n",
    "                data = np.load(pc_path)\n",
    "                coord_key = self.pc_keys[0]\n",
    "                color_key = self.pc_keys[1]\n",
    "\n",
    "                # Check if keys exist, proceed only if both are present\n",
    "                if coord_key in data and color_key in data:\n",
    "                    coords = data[coord_key]\n",
    "                    colors = data[color_key]\n",
    "\n",
    "                    # Basic check for non-empty arrays and correct second dimension\n",
    "                    if coords.ndim == 2 and coords.shape[1] == 3 and \\\n",
    "                       colors.ndim == 2 and colors.shape[1] == 3 and \\\n",
    "                       coords.shape[0] == colors.shape[0] and coords.shape[0] > 0:\n",
    "\n",
    "                        # Normalize colors if needed\n",
    "                        if colors.max() > 1.0:\n",
    "                            colors = colors / 255.0\n",
    "\n",
    "                        pointcloud = np.concatenate([coords.astype(np.float32),\n",
    "                                                     colors.astype(np.float32)], axis=1)\n",
    "\n",
    "                        # Subsample or pad points\n",
    "                        num_points = pointcloud.shape[0]\n",
    "                        if num_points == self.n_ctx:\n",
    "                            indices = np.arange(num_points)\n",
    "                        elif num_points > self.n_ctx:\n",
    "                            indices = np.random.choice(num_points, self.n_ctx, replace=False)\n",
    "                        else: # num_points < n_ctx\n",
    "                            indices = np.random.choice(num_points, self.n_ctx, replace=True)\n",
    "\n",
    "                        sampled_points = pointcloud[indices, :]\n",
    "                        pointcloud_tensor = torch.from_numpy(sampled_points).float().transpose(0, 1) # [6, n_ctx]\n",
    "                    # else: pointcloud_tensor remains zeros if shapes/keys are invalid\n",
    "\n",
    "            # else: pointcloud_tensor remains zeros if npz file not found\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print error for the specific item but allow dataloader to continue\n",
    "            # by returning the default zero tensors.\n",
    "            # WARNING: This might lead to training on invalid data if errors are frequent.\n",
    "            # Consider adding filtering logic or stricter error handling if needed.\n",
    "            print(f\"Warning: Error processing item for UID {uid}. Returning default tensors. Error: {e}\")\n",
    "            # traceback.print_exc() # Uncomment for full traceback during debugging\n",
    "\n",
    "\n",
    "        # --- Return Sample ---\n",
    "        # Always returns a dictionary, potentially with default zero tensors if errors occurred.\n",
    "        return {\n",
    "            'caption': caption,\n",
    "            'image': image_tensor,\n",
    "            'pointcloud': pointcloud_tensor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525bb898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Example Usage / Test Block ---\n",
    "# if __name__ == '__main__':\n",
    "#     print(\"\\n--- Testing ABODatasetSimplified ---\")\n",
    "\n",
    "#     # --- Configuration ---\n",
    "#     # !!! IMPORTANT: Set this to the correct path to your dataset !!!\n",
    "#     DATASET_ROOT = r'C:\\Users\\lvbab\\OneDrive\\Documents\\GitHub\\point-e\\point_e\\abo_integrated' # Use raw string for Windows paths\n",
    "#     TARGET_POINTS = 1024 # Or 4096, match your model's n_ctx\n",
    "#     BATCH_SIZE = 4\n",
    "#     NUM_WORKERS = 0 # Set > 0 for parallel loading if needed (use 0 for basic testing)\n",
    "\n",
    "#     # --- Get CLIP Preprocessor ---\n",
    "#     # You MUST provide a valid CLIP preprocessor function here.\n",
    "#     # Example: Load it using the 'clip' library (install if needed)\n",
    "#     clip_preprocess_func = None\n",
    "#     try:\n",
    "#         import clip\n",
    "#         # Load on CPU for this test setup phase\n",
    "#         clip_model, clip_preprocess_func = clip.load(\"ViT-L/14\", device=\"cpu\")\n",
    "#         print(\"Loaded CLIP preprocessor.\")\n",
    "#     except ImportError:\n",
    "#         print(\"Error: 'clip' library not found. Cannot get preprocessor.\")\n",
    "#         print(\"Please install it: pip install git+https://github.com/openai/CLIP.git\")\n",
    "#         # Exit or raise error if preprocessor is mandatory\n",
    "#         exit()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading CLIP: {e}\")\n",
    "#         exit()\n",
    "\n",
    "#     if clip_preprocess_func is None:\n",
    "#         print(\"CLIP preprocessor could not be loaded. Exiting.\")\n",
    "#         exit()\n",
    "\n",
    "#     # --- Instantiate Dataset ---\n",
    "#     print(f\"Initializing dataset from: {DATASET_ROOT}\")\n",
    "#     try:\n",
    "#         # Remember to adjust image_subdir and pc_keys if needed\n",
    "#         abo_dataset = ABODatasetSimplified(\n",
    "#             data_root=DATASET_ROOT,\n",
    "#             n_ctx=TARGET_POINTS,\n",
    "#             clip_preprocessor=clip_preprocess_func,\n",
    "#             image_subdir='',\n",
    "#             pc_keys=('coords', 'rgb')\n",
    "#         )\n",
    "\n",
    "#         # --- Create DataLoader ---\n",
    "#         if len(abo_dataset) > 0:\n",
    "#             data_loader = DataLoader(\n",
    "#                 dataset=abo_dataset,\n",
    "#                 batch_size=BATCH_SIZE,\n",
    "#                 shuffle=True,\n",
    "#                 num_workers=NUM_WORKERS\n",
    "#             )\n",
    "#             print(f\"Created DataLoader with {len(abo_dataset)} samples.\")\n",
    "\n",
    "#             # --- Load and Inspect One Batch ---\n",
    "#             print(\"\\nLoading one batch...\")\n",
    "#             try:\n",
    "#                 first_batch = next(iter(data_loader)) # Get the first batch\n",
    "\n",
    "#                 print(\"\\n--- Batch Data Inspection ---\")\n",
    "#                 # Check if keys exist before accessing\n",
    "#                 if 'caption' in first_batch:\n",
    "#                     print(f\"Captions in batch (first one): '{first_batch['caption'][0]}'\")\n",
    "#                 if 'image' in first_batch:\n",
    "#                     print(f\"Image tensor shape: {first_batch['image'].shape}\")\n",
    "#                     print(f\"Image tensor dtype: {first_batch['image'].dtype}\")\n",
    "#                 if 'pointcloud' in first_batch:\n",
    "#                     print(f\"Pointcloud tensor shape: {first_batch['pointcloud'].shape}\")\n",
    "#                     print(f\"Pointcloud tensor dtype: {first_batch['pointcloud'].dtype}\")\n",
    "#                 print(\"-----------------------------\")\n",
    "#                 print(\"DataLoader test successful!\")\n",
    "\n",
    "#             except StopIteration:\n",
    "#                  print(\"DataLoader is empty. Check dataset initialization.\")\n",
    "#             except Exception as e:\n",
    "#                  print(f\"Error loading batch from DataLoader: {e}\")\n",
    "#                  import traceback\n",
    "#                  traceback.print_exc()\n",
    "#         else:\n",
    "#             print(\"Dataset is empty. Cannot create or test DataLoader.\")\n",
    "\n",
    "#     except FileNotFoundError as e:\n",
    "#         print(f\"Error: {e}\") # Specifically catch file not found for jsonl\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error initializing ABODatasetSimplified: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfe418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Initializing model...\n",
      "Model config expects input_channels: 6\n",
      "Initializing with simple Linear fusion\n",
      "Model initialized.\n",
      "Initializing dataset and dataloader...\n",
      "Loaded CLIP preprocessor for ViT-L/14.\n",
      "Found 7890 UIDs in jsonl file.\n",
      "DataLoader created with 7890 samples.\n",
      "Number of trainable parameters: 1444864\n",
      "Setting up noise schedule...\n",
      "Using diffusion parameters: var_type=fixed_large, mean_type=epsilon, loss_type=mse, schedule=linear\n",
      "Using GaussianDiffusion with T=1024\n",
      "\n",
      "--- Starting Training ---\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 1/494 [00:38<5:12:30, 38.03s/it, loss=1.01]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 169\u001b[0m\n\u001b[0;32m    166\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(predicted_epsilon, epsilon)\n\u001b[0;32m    168\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 169\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    172\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\lvbab\\anaconda3\\envs\\meow\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lvbab\\anaconda3\\envs\\meow\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lvbab\\anaconda3\\envs\\meow\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\lvbab\\anaconda3\\envs\\meow\\Lib\\site-packages\\torch\\autograd\\function.py:292\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from point_e.models.fusion import TextImageFusionModule\n",
    "from point_e.models.multimodal import SimpleMultimodalTransformer\n",
    "from point_e.models.configs import MODEL_CONFIGS\n",
    "from point_e.models.download import load_checkpoint\n",
    "import traceback\n",
    "from point_e.diffusion.sampler import PointCloudSampler\n",
    "from point_e.diffusion.configs import DIFFUSION_CONFIGS\n",
    "from point_e.models.configs import MODEL_CONFIGS, model_from_config\n",
    "from point_e.models.download import load_checkpoint\n",
    "from point_e.util.plotting import plot_point_cloud\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from point_e.diffusion.gaussian_diffusion import GaussianDiffusion\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "try:\n",
    "    import clip\n",
    "except ImportError:\n",
    "    print(\"Warning: 'clip' library not found. CLIP preprocessing might fail.\")\n",
    "    print(\"Install using: pip install git+https://github.com/openai/CLIP.git\")\n",
    "    clip = None\n",
    "\n",
    "# == Device ==\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# == Model Initialization ==\n",
    "print(\"Initializing model...\")\n",
    "use_cross_attn = False  # or False, based on your preference\n",
    "model = SimpleMultimodalTransformer(\n",
    "    device=device,\n",
    "    dtype=torch.float32 if device == torch.device('cpu') else torch.float16,\n",
    "    cache_dir=\"./point_e_cache_train\",\n",
    "    use_cross_attention=use_cross_attn\n",
    ")\n",
    "model.to(device)\n",
    "print(\"Model initialized.\")\n",
    "\n",
    "# == Data ==\n",
    "print(\"Initializing dataset and dataloader...\")\n",
    "DATASET_ROOT = r'C:\\Users\\lvbab\\OneDrive\\Documents\\GitHub\\point-e\\point_e\\abo_integrated'\n",
    "TARGET_POINTS = 1024\n",
    "BATCH_SIZE = 16  # Adjust based on GPU memory\n",
    "NUM_WORKERS = 0  # Windows recommendation\n",
    "\n",
    "# Get CLIP Preprocessor\n",
    "clip_preprocess_func = None\n",
    "if clip:\n",
    "    try:\n",
    "        clip_name_to_load = model.clip_model_name if hasattr(model, 'clip_model_name') else \"ViT-L/14\"\n",
    "        _, clip_preprocess_func = clip.load(clip_name_to_load, device=\"cpu\", jit=False)\n",
    "        print(f\"Loaded CLIP preprocessor for {clip_name_to_load}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CLIP preprocessor: {e}. Exiting.\")\n",
    "        exit()\n",
    "else:\n",
    "    print(\"Error: 'clip' library not available. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "try:\n",
    "    dataset = ABODatasetSimplified(\n",
    "        data_root=DATASET_ROOT,\n",
    "        n_ctx=TARGET_POINTS,\n",
    "        clip_preprocessor=clip_preprocess_func,\n",
    "        pc_keys=('coords', 'colors')\n",
    "    )\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=(device == torch.device('cuda'))\n",
    "    )\n",
    "    print(f\"DataLoader created with {len(dataset)} samples.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating dataset/dataloader: {e}\")\n",
    "    traceback.print_exc()\n",
    "    exit()\n",
    "\n",
    "# == Optimizer ==\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in trainable_params)}\")\n",
    "if not trainable_params:\n",
    "    print(\"Warning: No trainable parameters found.\")\n",
    "learning_rate = 1e-4\n",
    "optimizer = AdamW(trainable_params, lr=learning_rate)\n",
    "\n",
    "# == Loss Function ==\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# == Noise Schedule ==\n",
    "print(\"Setting up noise schedule...\")\n",
    "noise_scheduler = None\n",
    "q_sample_fn = None\n",
    "try:\n",
    "    diffusion_steps = 1024\n",
    "    diffusion_config_name = 'base40M'\n",
    "    diffusion_kwargs = DIFFUSION_CONFIGS.get(diffusion_config_name, {})\n",
    "    model_var_type = diffusion_kwargs.get('model_var_type', 'fixed_large')\n",
    "    model_mean_type = diffusion_kwargs.get('model_mean_type', 'epsilon')\n",
    "    loss_type = diffusion_kwargs.get('loss_type', 'mse')\n",
    "    beta_schedule_name = diffusion_kwargs.get('beta_schedule', 'linear')\n",
    "    print(f\"Using diffusion parameters: var_type={model_var_type}, mean_type={model_mean_type}, loss_type={loss_type}, schedule={beta_schedule_name}\")\n",
    "\n",
    "    # Define linear beta schedule manually\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    betas = np.linspace(beta_start, beta_end, diffusion_steps)\n",
    "\n",
    "    init_args = {\n",
    "        'betas': betas,\n",
    "        'model_var_type': model_var_type,\n",
    "        'model_mean_type': model_mean_type,\n",
    "        'loss_type': loss_type,\n",
    "    }\n",
    "    noise_scheduler = GaussianDiffusion(**init_args)\n",
    "    print(f\"Using GaussianDiffusion with T={noise_scheduler.num_timesteps}\")\n",
    "    q_sample_fn = noise_scheduler.q_sample\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up noise schedule: {e}\")\n",
    "    traceback.print_exc()\n",
    "    raise RuntimeError(\"Noise scheduler setup failed\")\n",
    "\n",
    "# Ensure scheduler is initialized\n",
    "if noise_scheduler is None or q_sample_fn is None:\n",
    "    print(\"Error: Noise scheduler setup failed. Exiting.\")\n",
    "    raise RuntimeError(\"Noise scheduler is not initialized\")\n",
    "\n",
    "# == Training Parameters ==\n",
    "epochs = 100\n",
    "save_interval = 10\n",
    "output_dir = \"./checkpoints_multimodal\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# == Training Loop ==\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    total_loss = 0.0\n",
    "    steps_in_epoch = 0\n",
    "    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for step, batch in progress_bar:\n",
    "        try:\n",
    "            captions = batch['caption']\n",
    "            images = batch['image'].to(device, dtype=model.dtype)\n",
    "            x_0 = batch['pointcloud'].to(device, dtype=model.dtype)\n",
    "\n",
    "            if torch.all(x_0 == 0) or torch.all(images == 0):\n",
    "                continue\n",
    "\n",
    "            current_batch_size = x_0.shape[0]\n",
    "            t = torch.randint(0, noise_scheduler.num_timesteps, (current_batch_size,), device=device).long()\n",
    "            epsilon = torch.randn_like(x_0)\n",
    "            x_t = q_sample_fn(x_start=x_0, t=t, noise=epsilon)\n",
    "\n",
    "            predicted_epsilon = model(x=x_t, t=t, images=images, texts=captions)\n",
    "            loss = loss_fn(predicted_epsilon, epsilon)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            steps_in_epoch += 1\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during training step {step} for epoch {epoch+1}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    if steps_in_epoch > 0:\n",
    "        avg_loss = total_loss / steps_in_epoch\n",
    "        print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f} ({steps_in_epoch}/{len(data_loader)} steps)\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1} finished. No batches processed successfully.\")\n",
    "\n",
    "    if (epoch + 1) % save_interval == 0 or epoch == epochs - 1:\n",
    "        save_path = os.path.join(output_dir, f\"model_epoch_{epoch+1}.pt\")\n",
    "        save_dict = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss if steps_in_epoch > 0 else float('inf'),\n",
    "            'use_cross_attention': use_cross_attn\n",
    "        }\n",
    "        torch.save(save_dict, save_path)\n",
    "        print(f\"Model checkpoint saved to {save_path}\")\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc4db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# import os\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# from point_e.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config\n",
    "# from point_e.models.multimodal import SimpleMultimodalTransformer  # assumes multimodal.py is in PYTHONPATH\n",
    "\n",
    "# # TODO: replace this with your real dataset class or import\n",
    "# class MultimodalPointCloudDataset(torch.utils.data.Dataset):\n",
    "#     \"\"\"Simple placeholder dataset.\n",
    "#     Expects each sample as a dict with keys:\n",
    "#         'points'  : torch.Tensor [C, N]  (original clean point cloud)\n",
    "#         'images'  : List[PIL.Image] or a single PIL.Image for conditioning\n",
    "#         'captions': str (text prompt)\n",
    "#     \"\"\"\n",
    "#     def __init__(self, root: str, split: str = \"train\"):\n",
    "#         super().__init__()\n",
    "#         # load your metadata here\n",
    "#         self.meta = []  # list of file paths / annotations\n",
    "#         # ...\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.meta)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # load point cloud, image and caption\n",
    "#         raise NotImplementedError(\"Implement your dataset loading logic here\")\n",
    "\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     \"\"\"Keeps images & captions as Python lists so CLIP can handle them.\"\"\"\n",
    "#     points = torch.stack([b[\"points\"] for b in batch], dim=0)  # [B, C, N]\n",
    "#     images = [b[\"images\"] for b in batch]\n",
    "#     captions = [b[\"captions\"] for b in batch]\n",
    "#     return {\"points\": points, \"images\": images, \"captions\": captions}\n",
    "\n",
    "\n",
    "# def parse_args():\n",
    "#     p = argparse.ArgumentParser(\"Train multimodal fusion (Point‑E)\")\n",
    "#     p.add_argument(\"--data\", required=True, help=\"Dataset root directory\")\n",
    "#     p.add_argument(\"--epochs\", type=int, default=50)\n",
    "#     p.add_argument(\"--batch_size\", type=int, default=8)\n",
    "#     p.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "#     p.add_argument(\"--outdir\", default=\"checkpoints\")\n",
    "#     p.add_argument(\"--cache_dir\", default=None, help=\"CLIP/Point‑E cache\")\n",
    "#     p.add_argument(\"--use_cross_attention\", action=\"store_true\")\n",
    "#     p.add_argument(\"--mixed_precision\", action=\"store_true\")\n",
    "#     return p.parse_args()\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     args = parse_args()\n",
    "\n",
    "#     # Device & dtype --------------------------------------------------------\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dtype = torch.float16 if args.mixed_precision else torch.float32\n",
    "\n",
    "#     # Data ------------------------------------------------------------------\n",
    "#     train_ds = MultimodalPointCloudDataset(args.data, split=\"train\")\n",
    "#     train_loader = DataLoader(\n",
    "#         train_ds,\n",
    "#         batch_size=args.batch_size,\n",
    "#         shuffle=True,\n",
    "#         num_workers=4,\n",
    "#         pin_memory=True,\n",
    "#         drop_last=True,\n",
    "#         collate_fn=collate_fn,\n",
    "#     )\n",
    "\n",
    "#     # Diffusion process -----------------------------------------------------\n",
    "#     diffusion = diffusion_from_config(DIFFUSION_CONFIGS[\"base\"],)  # base schedule\n",
    "#     diffusion = diffusion.to(device)\n",
    "\n",
    "#     # Model -----------------------------------------------------------------\n",
    "#     model = SimpleMultimodalTransformer(\n",
    "#         device=device,\n",
    "#         dtype=dtype,\n",
    "#         cache_dir=args.cache_dir,\n",
    "#         use_cross_attention=args.use_cross_attention,\n",
    "#     ).to(device)\n",
    "\n",
    "#     # Optimizer -------------------------------------------------------------\n",
    "#     optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)\n",
    "#     scaler = torch.cuda.amp.GradScaler(enabled=args.mixed_precision)\n",
    "\n",
    "#     os.makedirs(args.outdir, exist_ok=True)\n",
    "#     global_step = 0\n",
    "\n",
    "#     # Training loop ---------------------------------------------------------\n",
    "#     for epoch in range(1, args.epochs + 1):\n",
    "#         model.train()\n",
    "#         for batch in train_loader:\n",
    "#             pc0 = batch[\"points\"].to(device)         # x_0\n",
    "#             images = batch[\"images\"]                 # list of PIL.Image\n",
    "#             captions = batch[\"captions\"]             # list[str]\n",
    "\n",
    "#             bs = pc0.size(0)\n",
    "#             t = torch.randint(0, diffusion.num_timesteps, (bs,), device=device, dtype=torch.long)\n",
    "#             noise = torch.randn_like(pc0)\n",
    "#             x_t = diffusion.q_sample(pc0, t, noise=noise)\n",
    "\n",
    "#             with torch.cuda.amp.autocast(enabled=args.mixed_precision):\n",
    "#                 pred_noise = model(x_t, t, images=images, texts=captions)\n",
    "#                 loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.unscale_(optim)\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#             scaler.step(optim)\n",
    "#             scaler.update()\n",
    "#             optim.zero_grad(set_to_none=True)\n",
    "\n",
    "#             if global_step % 100 == 0:\n",
    "#                 print(f\"Epoch {epoch} | Step {global_step} | Loss {loss.item():.4f}\")\n",
    "#             global_step += 1\n",
    "\n",
    "#         # Save checkpoint ----------------------------------------------------\n",
    "#         ckpt_path = os.path.join(args.outdir, f\"epoch_{epoch:03d}.pt\")\n",
    "#         torch.save({\n",
    "#             \"model\": model.state_dict(),\n",
    "#             \"optimizer\": optim.state_dict(),\n",
    "#             \"epoch\": epoch,\n",
    "#             \"global_step\": global_step,\n",
    "#         }, ckpt_path)\n",
    "#         print(f\"Saved checkpoint to {ckpt_path}\")\n",
    "\n",
    "#     print(\"Training complete.\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
