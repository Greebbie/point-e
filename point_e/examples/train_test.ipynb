{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ca893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from point_e.models.fusion import TextImageFusionModule\n",
    "from point_e.models.multimodal import MultimodalPointDiffusionTransformer\n",
    "from point_e.models.configs import MODEL_CONFIGS\n",
    "from point_e.models.download import load_checkpoint\n",
    "\n",
    "from point_e.diffusion.sampler import PointCloudSampler\n",
    "from point_e.diffusion.configs import DIFFUSION_CONFIGS\n",
    "from point_e.models.configs import MODEL_CONFIGS, model_from_config\n",
    "from point_e.models.download import load_checkpoint\n",
    "from point_e.util.plotting import plot_point_cloud\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# CLIP preprocessing transform\n",
    "def get_clip_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                             (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "\n",
    "def main():\n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create the base multimodal model\n",
    "    model_config = MODEL_CONFIGS['base40M'].copy()\n",
    "    base_model = MultimodalPointDiffusionTransformer(\n",
    "        device=device,\n",
    "        **model_config\n",
    "    )\n",
    "    base_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['base40M'])\n",
    "    \n",
    "    # Create the upsampler model (original Point-E)\n",
    "    upsampler_model = model_from_config(MODEL_CONFIGS['upsample'], device)\n",
    "    upsampler_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['upsample'])\n",
    "    \n",
    "    # Load checkpoints\n",
    "    base_model.load_state_dict(torch.load(\"multimodal_point_e_final.pt\", map_location=device))\n",
    "    upsampler_model.load_state_dict(load_checkpoint('upsample', device))\n",
    "    \n",
    "    # Set models to eval mode\n",
    "    base_model.eval()\n",
    "    upsampler_model.eval()\n",
    "    \n",
    "    # Create sampler\n",
    "    sampler = PointCloudSampler(\n",
    "        device=device,\n",
    "        models=[base_model, upsampler_model],\n",
    "        diffusions=[base_diffusion, upsampler_diffusion],\n",
    "        num_points=[1024, 4096 - 1024],\n",
    "        aux_channels=['R', 'G', 'B'],\n",
    "        guidance_scale=[3.0, 3.0],\n",
    "    )\n",
    "    \n",
    "    # Prepare inputs\n",
    "    transform = get_clip_transform()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image_path = \"example_image.jpg\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Text prompt\n",
    "    text_prompt = \"a red motorcycle with chrome details\"\n",
    "    \n",
    "    # Generate point cloud\n",
    "    print(\"Generating point cloud...\")\n",
    "    samples = None\n",
    "    with torch.no_grad():\n",
    "        for x in tqdm(sampler.sample_batch_progressive(\n",
    "            batch_size=1, \n",
    "            model_kwargs=dict(images=image, texts=[text_prompt])\n",
    "        )):\n",
    "            samples = x\n",
    "    \n",
    "    # Convert to point cloud\n",
    "    pc = sampler.output_to_point_clouds(samples)[0]\n",
    "    \n",
    "    # Visualize\n",
    "    fig = plot_point_cloud(pc, grid_size=1)\n",
    "    fig.savefig(\"multimodal_output.png\")\n",
    "    \n",
    "    # Save point cloud\n",
    "    pc.save(\"multimodal_output.npz\")\n",
    "    \n",
    "    print(f\"Generated point cloud saved to multimodal_output.npz\")\n",
    "    print(f\"Visualization saved to multimodal_output.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904565ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 测试融合模块\n",
    "def test_fusion_module():\n",
    "    print(\"Testing TextImageFusionModule...\")\n",
    "    fusion = TextImageFusionModule(clip_dim=768, fusion_dim=512)\n",
    "    \n",
    "    # 创建随机输入\n",
    "    batch_size = 2\n",
    "    text_emb = torch.randn(batch_size, 768)\n",
    "    img_tokens = torch.randn(batch_size, 196, 768)  # 196 = 14x14 grid\n",
    "    \n",
    "    # 前向传递\n",
    "    out = fusion(text_emb, img_tokens)\n",
    "    \n",
    "    print(f\"Input shapes: text_emb {text_emb.shape}, img_tokens {img_tokens.shape}\")\n",
    "    print(f\"Output shape: {out.shape}\")\n",
    "    print(\"Fusion module test passed!\")\n",
    "    \n",
    "# 测试多模态模型\n",
    "def test_multimodal_model():\n",
    "    print(\"Testing MultimodalPointDiffusionTransformer...\")\n",
    "    device = torch.device('cpu')  # 使用CPU以便调试\n",
    "    \n",
    "    # 创建模型\n",
    "    model_config = MODEL_CONFIGS['base40M'].copy()\n",
    "    try:\n",
    "        model = MultimodalPointDiffusionTransformer(\n",
    "            device=device,\n",
    "            dtype=torch.float32,\n",
    "            **model_config\n",
    "        )\n",
    "        print(\"Model initialization successful!\")\n",
    "        \n",
    "        # 测试前向传递\n",
    "        batch_size = 2\n",
    "        x = torch.randn(batch_size, 6, 1024)  # [B, C, n_ctx]\n",
    "        t = torch.randint(0, 1000, (batch_size,))\n",
    "        \n",
    "        # 创建随机文本和图像\n",
    "        from point_e.models.pretrained_clip import ImageCLIP\n",
    "        dummy_img = torch.randn(batch_size, 3, 224, 224)\n",
    "        dummy_texts = [\"a red chair\", \"a blue table\"]\n",
    "        \n",
    "        print(\"Running forward pass...\")\n",
    "        out = model(x, t, images=[dummy_img]*batch_size, texts=dummy_texts)\n",
    "        \n",
    "        print(f\"Input shapes: x {x.shape}, t {t.shape}\")\n",
    "        print(f\"Output shape: {out.shape}\")\n",
    "        print(\"Multimodal model test passed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in model test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_fusion_module()\n",
    "    test_multimodal_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6289d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from point_e.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config\n",
    "\n",
    "def test_training_loop():\n",
    "    print(\"Testing training loop...\")\n",
    "    device = torch.device('cpu')  # Use CPU for debugging\n",
    "    \n",
    "    # Create model (reuse from previous test)\n",
    "    from point_e.models.multimodal import MultimodalPointDiffusionTransformer\n",
    "    from point_e.models.configs import MODEL_CONFIGS\n",
    "    \n",
    "    model_config = MODEL_CONFIGS['base40M'].copy()\n",
    "    model = MultimodalPointDiffusionTransformer(\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "        **model_config\n",
    "    )\n",
    "    diffusion = diffusion_from_config(DIFFUSION_CONFIGS['base40M'])\n",
    "    \n",
    "    # Freeze all parameters first, then unfreeze fusion module\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if \"fusion_module\" in name or \"clip_embed\" in name:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # Setup optimizer (only for trainable parameters)\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=1e-4,\n",
    "    )\n",
    "    \n",
    "    # Create dummy batch\n",
    "    batch_size = 2\n",
    "    images = torch.randn(batch_size, 3, 224, 224)\n",
    "    texts = [\"a red chair\", \"a blue table\"]\n",
    "    point_clouds = torch.randn(batch_size, 6, 1024)  # [B, C, n_ctx]\n",
    "    \n",
    "    # Training step\n",
    "    model.train()\n",
    "    t = torch.randint(0, diffusion.num_timesteps, (batch_size,), device=device)\n",
    "    model_kwargs = {\"images\": images, \"texts\": texts}\n",
    "    \n",
    "    try:\n",
    "        # Compute training loss\n",
    "        losses = diffusion.training_losses(\n",
    "            model=model,\n",
    "            x_start=point_clouds,\n",
    "            t=t,\n",
    "            model_kwargs=model_kwargs\n",
    "        )\n",
    "        loss = losses[\"loss\"].mean()\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Training loss: {loss.item()}\")\n",
    "        print(\"Training loop test passed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in training loop test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b78cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_generation():\n",
    "    print(\"Testing point cloud generation...\")\n",
    "    device = torch.device('cpu')  # Use CPU for debugging\n",
    "    \n",
    "    # Create models\n",
    "    from point_e.diffusion.sampler import PointCloudSampler\n",
    "    from point_e.diffusion.configs import DIFFUSION_CONFIGS\n",
    "    from point_e.models.configs import MODEL_CONFIGS, model_from_config\n",
    "    from point_e.models.multimodal import MultimodalPointDiffusionTransformer\n",
    "    \n",
    "    # Create the base multimodal model\n",
    "    model_config = MODEL_CONFIGS['base40M'].copy()\n",
    "    base_model = MultimodalPointDiffusionTransformer(\n",
    "        device=device,\n",
    "        **model_config\n",
    "    )\n",
    "    base_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['base40M'])\n",
    "    \n",
    "    # Create the upsampler model (original Point-E)\n",
    "    upsampler_model = model_from_config(MODEL_CONFIGS['upsample'], device)\n",
    "    upsampler_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['upsample'])\n",
    "    \n",
    "    # Set models to eval mode\n",
    "    base_model.eval()\n",
    "    upsampler_model.eval()\n",
    "    \n",
    "    # Create sampler\n",
    "    sampler = PointCloudSampler(\n",
    "        device=device,\n",
    "        models=[base_model, upsampler_model],\n",
    "        diffusions=[base_diffusion, upsampler_diffusion],\n",
    "        num_points=[1024, 4096 - 1024],\n",
    "        aux_channels=['R', 'G', 'B'],\n",
    "        guidance_scale=[3.0, 3.0],\n",
    "    )\n",
    "    \n",
    "    # Create dummy inputs\n",
    "    image = torch.randn(1, 3, 224, 224)\n",
    "    text_prompt = \"a red chair\"\n",
    "    \n",
    "    try:\n",
    "        # Generate point cloud (with limited steps for testing)\n",
    "        print(\"Generating point cloud...\")\n",
    "        with torch.no_grad():\n",
    "            # Use a very small number of steps for testing\n",
    "            base_diffusion.num_timesteps = 10\n",
    "            upsampler_diffusion.num_timesteps = 10\n",
    "            \n",
    "            samples = None\n",
    "            for x in sampler.sample_batch_progressive(\n",
    "                batch_size=1, \n",
    "                model_kwargs=dict(images=[image], texts=[text_prompt])\n",
    "            ):\n",
    "                samples = x\n",
    "        \n",
    "        print(\"Generation test passed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in generation test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_generation()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
